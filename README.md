# Taller AI Generativa
Este repositorio contiene el material y el c√≥digo del taller ‚ÄúOptimizaci√≥n de la Atenci√≥n al Cliente en E-commerce con IA Generativa‚Äù, basado en el caso de estudio de la empresa ficticia "EcoMarket".


# Fase 1 ‚Äì Selecci√≥n y Justificaci√≥n del Modelo de IA

## 1Ô∏è‚É£ Tipo de modelo propuesto
Optar√≠a por un **LLM de prop√≥sito general** , en este caso el *Llama 3* combinado con un enfoque de **Retrieval-Augmented Generation (RAG)**.

- **Motivo principal:** La mayor√≠a de las consultas (‚âà 80 %) son repetitivas y dependen de informaci√≥n actualizada del negocio (estado de pedidos, pol√≠ticas de devoluci√≥n, cat√°logo).  
  Un RAG permite que el modelo base no requiera reentrenamiento cada vez que cambian los datos: solo necesita conectarse a una base de conocimiento din√°mica.


## 2Ô∏è‚É£ Por qu√© la elecci√≥n del modelo
- **Precisi√≥n y fluidez:** Un LLM grande ofrece lenguaje natural y tono emp√°tico para responder en m√∫ltiples canales (chat, email, redes sociales).
- **Costo / eficiencia:** En lugar de un *fine-tuned* propietario ‚Äîque implicar√≠a entrenamiento continuo y costo elevado‚Äî el enfoque RAG permite usar el modelo tal cual, complementado con datos internos indexados.
- **Escalabilidad:** El mismo modelo puede atender picos de miles de consultas diarias mediante *autoscaling* en la nube.
- **Privacidad:** Es posible tener un control total de los datos al usar con una infraestructura propia en la nube.


## 3Ô∏è‚É£ Arquitectura propuesta  

<img src="https://github.com/user-attachments/assets/125198ab-9ede-4245-86e2-b36eff960e99" width="450" alt="Diagrama del flujo" />

## ‚öôÔ∏è Tecnolog√≠as Seleccionadas y Justificaci√≥n

### üß© Gateway + API
El **gateway** gestiona el tr√°fico y la seguridad; la **API (FastAPI)** recibe la solicitud y la pasa al orquestador.

- Esta separaci√≥n permite **escalar** y **asegurar** el sistema, dado el r√°pido crecimuiento se recomienda separar estos roles.

### üîó LangChain + FastAPI
Combina la **orquestaci√≥n de flujos de IA** de **LangChain** con la **velocidad y facilidad de despliegue** de **FastAPI**, ofreciendo una API robusta, escalable y de alta disponibilidad para integrar el modelo con los distintos canales de comunicaci√≥n.

### üóÇÔ∏è FAISS
Librer√≠a **open source** optimizada para **b√∫squedas vectoriales r√°pidas**.  
Permite mantener **control total de los datos** y **reducir costos**, ideal para gestionar el √≠ndice sem√°ntico de productos y pedidos sin depender de servicios de terceros.

### ü§ñ Llama 3
Modelo de lenguaje **open source** de alto rendimiento que brinda **privacidad y personalizaci√≥n** al ejecutarse en una nube privada.  
Ofrece **calidad de generaci√≥n cercana a GPT-4** con la flexibilidad de *fine-tuning* y **costos predecibles**.


- **Integraci√≥n con base de datos:**  
  - Cat√°logo de productos, informaci√≥n de env√≠os y estados de pedido se almacenan en una base de datos relacional/NoSQL y se indexan en un motor vectorial para consultas en tiempo real.  
  - El modelo consulta estos datos mediante RAG antes de redactar la respuesta.
- **Modo de operaci√≥n:**  
  - Para preguntas de estado de pedido, una capa de l√≥gica de negocio valida identidad y obtiene datos exactos.  
  - Para consultas complejas (20 %), el sistema deriva autom√°ticamente a un agente humano, entregando el contexto de la conversaci√≥n.


## 4Ô∏è‚É£ Justificaci√≥n Final

- **Costo:**  
  Uso de un **LLM open source (Llama 3)** junto con un √≠ndice vectorial **FAISS** permite **evitar tarifas por token** y mantener **costos predecibles**.  
  Se paga √∫nicamente por la infraestructura en la nube, que puede dimensionarse seg√∫n la demanda.

- **Escalabilidad:**  
  La arquitectura combina **Gateway + API (FastAPI)** con un orquestador en **LangChain**, facilitando el *autoscaling* en la nube.  
  Esto permite manejar miles de consultas simult√°neas, con **balanceo de carga** y **cacheo de respuestas frecuentes** para optimizar recursos.

- **Mantenibilidad:**  
  La base de conocimiento se actualiza din√°micamente en **FAISS**, sin necesidad de reentrenar el modelo.  
  La separaci√≥n de capas (Gateway, API, Orquestador y Vector DB) simplifica actualizaciones y despliegues continuos.

- **Privacidad y Control de Datos:**  
  **Llama 3** puede ejecutarse en una nube privada, asegurando que las conversaciones y datos de clientes permanezcan bajo control total, clave para cumplir regulaciones.

- **Calidad de Respuesta:**  
  **Llama 3**, integrado con LangChain, ofrece **lenguaje natural, empat√≠a y soporte multicanal**, garantizando interacciones de alta calidad.  
  Adem√°s, el orquestador identifica consultas complejas y **deriva autom√°ticamente a un agente humano** con el contexto completo para una atenci√≥n sin fricciones.

---


# Fase 2 ‚Äì Evaluaci√≥n de Fortalezas, Limitaciones y Riesgos √âticos

### üí™ Fortalezas
- **Reducci√≥n del tiempo de respuesta:**  
  El sistema puede responder en segundos, disminuyendo el promedio de 24 horas a casi tiempo real.
- **Disponibilidad 24/7:**  
  La arquitectura en la nube permite atender consultas sin interrupciones, incluso en picos de demanda.
- **Cobertura de consultas repetitivas (~80 %):**  
  El orquestador con RAG (LangChain + FAISS) recupera informaci√≥n precisa del cat√°logo y pedidos de EcoMarket.
- **Escalabilidad y costos controlados:**  
  FAISS y Llama 3 autohospedado ofrecen independencia de tarifas por token y facilitan el crecimiento seg√∫n demanda.
- **Privacidad y control de datos:**  
  Al ejecutar Llama 3 en infraestructura privada, EcoMarket mantiene la propiedad de los datos sensibles.

### ‚ö†Ô∏è Limitaciones
- **Casos complejos (~20 %):**  
  Requieren empat√≠a y juicio humano (quejas graves, conflictos de reembolso). 
- **Dependencia de la base de conocimiento:**  
  Si FAISS contiene informaci√≥n desactualizada o err√≥nea, el modelo puede devolver respuestas incorrectas.
- **Mantenimiento de infraestructura:**  
  Ejecutar Llama 3 en la nube implica monitoreo de GPUs, actualizaciones de seguridad y optimizaci√≥n de costos.
- **Idioma y matices culturales:**  
  Aunque Llama 3 es multiling√ºe, podr√≠a cometer errores sutiles en expresiones locales o tonos espec√≠ficos.

### üõë Riesgos √âticos

1. **Alucinaciones:**  
   El modelo podr√≠a inventar informaci√≥n sobre pedidos o caracter√≠sticas de productos.  
   - *Mitigaci√≥n:* Validar datos cr√≠ticos (estado de pedido, precios) con reglas de negocio antes de enviar la respuesta.

2. **Sesgo:**  
   Los datos de entrenamiento pueden contener sesgos que generen respuestas preferenciales o discriminatorias.  
   - *Mitigaci√≥n:* Monitoreo constante, pruebas de equidad y ajuste de *prompts*.

3. **Privacidad de Datos:**  
   El sistema maneja direcciones, historial de compras y datos personales.  
   - *Mitigaci√≥n:* Cifrado en tr√°nsito y reposo, anonimizaci√≥n de logs, control estricto de acceso, y no-retenci√≥n en servicios externos.

4. **Impacto Laboral:**  
   La automatizaci√≥n podr√≠a reducir la necesidad de agentes humanos.  
   - *Mitigaci√≥n:* Enfocar el proyecto en **empoderar** a los agentes, delegando en la IA las tareas repetitivas y permitiendo que el personal se centre en casos complejos o de alto valor.

---

**Conclusi√≥n:**  
La soluci√≥n basada en **LangChain + FastAPI, FAISS y Llama 3** es potente para reducir tiempos de respuesta y manejar la mayor√≠a de las consultas.  
Sin embargo, requiere una estrategia clara de **supervisi√≥n humana, gobernanza de datos y gesti√≥n del cambio** para mitigar riesgos √©ticos y preservar la calidad del servicio.


# Fase 3 ¬∑ Sistema de consulta de pedidos

## Resumen
Aplicaci√≥n web con asistente conversacional para consultar estados de pedidos. Combina:
- Interfaz de chat en Streamlit para clientes.
- Motor conversacional que usa b√∫squeda vectorial (FAISS + Sentence Transformers) y generaci√≥n con Llama 3 v√≠a Ollama.

## Arquitectura

| Componente | Descripci√≥n |
|------------|-------------|
| `start.py` | Orquesta la ejecuci√≥n local: verifica Docker, reconstruye (si corresponde) la imagen `pedidos-app` y lanza el contenedor con los puertos 8501 (Streamlit) y 11434 (Ollama). |
| `check_docker.py` | Diagn√≥stico independiente: valida instalaci√≥n/estado de Docker y disponibilidad de archivos clave; puede ejecutar `docker build --dry-run`. |
| `Dockerfile` | Imagen basada en `python:3.11-slim`; instala compiladores/bibliotecas para FAISS y NumPy, a√±ade Ollama, prepara dependencias y configura `entrypoint.sh`. |
| `entrypoint.sh` | Dentro del contenedor: inicia Ollama, espera disponibilidad, descarga el modelo `llama3` si falta y lanza la app de Streamlit. |
| `requirements.txt` | Dependencias fijadas para FAISS, Sentence Transformers, Ollama, Streamlit y utilidades de Hugging Face. |
| `src/streamlit_app.py` | UI del chat: administra estado en `st.session_state`, detecta n√∫meros de seguimiento, prepara contexto y llama a `ollama.chat` para responder. |
| `src/settings.toml` | Configuraci√≥n del modelo (nombre, temperatura) y prompts/instrucciones para diferenciar consultas de seguimiento y devoluciones. |
| `src/ingest_data.py` | Pipeline de indexaci√≥n: lee `data/pedidos.json`, genera descripciones enriquecidas, calcula embeddings (`all-MiniLM-L6-v2`), crea el √≠ndice FAISS y persiste metadatos. |
| `data/pedidos.json` | 30 pedidos de ejemplo con estado, fechas, destino, transportista, enlaces, productos y pol√≠ticas de devoluci√≥n. |

## Flujo operativo

1. Ejecutar `python start.py` (o pasos manuales equivalentes).
2. El script valida Docker y reconstruye/lanzar el contenedor `pedidos-app`.
3. El contenedor inicia Ollama, garantiza la disponibilidad del modelo `llama3` y arranca la aplicaci√≥n Streamlit en `http://localhost:8501`.
4. La UI permite consultar pedidos; al detectar un n√∫mero v√°lido, busca contexto en el √≠ndice FAISS y genera respuestas emp√°ticas y estructuradas seg√∫n `settings.toml`.

## Datos y modelo

- **Datos:** `data/pedidos.json` alimenta la indexaci√≥n y la UI.
- **Modelo conversacional:** Llama 3 servido por Ollama, configurado mediante `settings.toml`.

## Prompts utilizados

[prompts]

# -------------------------------------------------------------------
# Rol del modelo
# -------------------------------------------------------------------
role_prompt = """
Eres un agente virtual de servicio al cliente altamente capacitado,
especializado en seguimiento de pedidos y gesti√≥n de devoluciones.

Tu objetivo es brindar una experiencia de atenci√≥n confiable y emp√°tica:

- Lee cuidadosamente la informaci√≥n del pedido y su lista de productos.
- Proporciona detalles claros y completos: n√∫mero de seguimiento, estado actual,
  fecha estimada o real de entrega, destino, transportadora y enlace de rastreo.
- Explica cualquier incidencia (retrasos, cancelaciones, aduanas, etc.) con un
  tono profesional, cercano y comprensible.
- **Jam√°s inventes datos**: usa √∫nicamente la informaci√≥n provista.
- **No inventes nombres de clientes ni de productos.**
  - Si no se proporciona el nombre del cliente, usa un saludo neutro.
  - Para los productos, menciona solo los nombres tal como aparezcan en la informaci√≥n del pedido,
    sin abreviaciones ni creaciones propias.
- Mant√©n siempre un lenguaje amable, natural y cordial, evitando tecnicismos innecesarios.
- Adec√∫a la longitud del mensaje: breve y directo, pero con la informaci√≥n completa.
- Si la consulta no es clara, pide amablemente una aclaraci√≥n.
"""

# -------------------------------------------------------------------
# Instrucciones principales
# -------------------------------------------------------------------
instruction_prompt = """
Analiza el contenido comprendido entre >>>>>CONTENIDO<<<<<.

El usuario puede:
1) Consultar el estado de un pedido.
2) Solicitar informaci√≥n sobre la devoluci√≥n de productos.

Debes:

1. Detectar con precisi√≥n si la intenci√≥n principal del usuario es:
   a) Conocer el estado de un pedido, o
   b) Obtener informaci√≥n sobre la devoluci√≥n de productos.

2. Para **estado de pedido**:
   - Saluda de forma personalizada solo si el nombre del cliente est√° expl√≠citamente disponible;
     de lo contrario, usa un saludo neutro (por ejemplo, "¬°Hola!").
   - Informa de manera clara el n√∫mero de seguimiento, estado actual,
     fecha estimada o real de entrega, destino, transportadora y, si existe,
     el enlace de rastreo.
   - Si hay retrasos, explica brevemente el motivo y ofrece disculpas emp√°ticas.
   - Cierra con un mensaje de agradecimiento u ofrecimiento de ayuda adicional.

3. Para **devoluci√≥n de productos**:
   - Saluda cordialmente, sin inventar nombre del cliente.
   - Confirma el n√∫mero de seguimiento.
   - Revisa cada producto y menciona **exactamente** el nombre provisto en la informaci√≥n del pedido,
     indicando si la devoluci√≥n est√° permitida o no.  
     En caso de negativa, explica el motivo (ej.: perecedero, higiene, pol√≠tica de devoluci√≥n).
   - Finaliza con una breve orientaci√≥n sobre los siguientes pasos
     (por ejemplo, c√≥mo iniciar el tr√°mite de devoluci√≥n de los productos aceptados).

4. Si el pedido no existe o el n√∫mero de seguimiento no se encuentra:
   - Responde con empat√≠a y explica que no se hall√≥ informaci√≥n.
   - Invita a verificar el n√∫mero o proporcionar m√°s datos.

5. Estilo de respuesta:
   - Usa un lenguaje conversacional, c√°lido y profesional.
   - No entregues la respuesta en JSON ni en estructuras de datos.
   - Mant√©n la coherencia gramatical y un flujo natural,
     como si fueras un agente humano.
   - **Nunca infieras ni generes nombres de personas o productos que no aparezcan
     de forma expl√≠cita en los datos.**

6. Si el usuario env√≠a varias preguntas (seguimiento y devoluci√≥n a la vez):
   - Contesta de forma ordenada, cubriendo ambos temas en un solo mensaje,
     manteniendo claridad y cortes√≠a.

Recuerda: la respuesta final debe ser un **mensaje fluido y natural**,
sin claves ni formato t√©cnico y respetando los nombres exactos provistos.
"""

# -------------------------------------------------------------------
# Ejemplo de seguimiento (positivo)
# -------------------------------------------------------------------
positive_example = """
[Cliente] 2025-09-20: Hola, quiero saber el estado de mi pedido 20002.
[Agente]  2025-09-20: ¬°Hola! Con gusto te ayudo. El pedido 20002 fue entregado
el 22 de septiembre de 2025 en Medell√≠n, Colombia por FedEx.
Puedes ver el detalle en: https://www.fedex.com/fedextrack/?trknbr=20002.
¬°Gracias por tu compra y que disfrutes tu producto!
"""

# -------------------------------------------------------------------
# Ejemplo de devoluci√≥n (mixta)
# -------------------------------------------------------------------
return_example = """
[Cliente] 2025-10-01: Quiero devolver los productos del pedido 20004.
[Agente]  2025-10-01: Claro, ya revis√© el pedido 20004. El "Yogur griego"
no puede devolverse porque es un alimento perecedero.
Si tienes m√°s productos que desees devolver y cumplen las pol√≠ticas,
con gusto te ayudo a iniciar el proceso. Gracias por tu comprensi√≥n.
"""

# -------------------------------------------------------------------
# Ejemplo de seguimiento con retraso
# -------------------------------------------------------------------
delay_example = """
[Cliente] 2025-10-05: Buen d√≠a, ¬øqu√© pasa con mi pedido 30007?
[Agente]  2025-10-05: ¬°Hola! Revis√© el pedido 30007 y actualmente se encuentra
en tr√°nsito con la transportadora DHL. La entrega estimada era el 4 de octubre,
pero se retras√≥ por condiciones clim√°ticas en la ruta.
La nueva fecha estimada de entrega es el 7 de octubre de 2025.
Te pedimos disculpas por el inconveniente y agradecemos tu paciencia.
Puedes hacer seguimiento aqu√≠: https://www.dhl.com/track?num=30007.
"""

# -------------------------------------------------------------------
# Ejemplo de pedido no encontrado
# -------------------------------------------------------------------
not_found_example = """
[Cliente] 2025-10-08: Necesito saber el estado del pedido 99999.
[Agente]  2025-10-08: Hola, intent√© ubicar el pedido 99999, pero no encontr√©
informaci√≥n en nuestro sistema. Por favor verifica que el n√∫mero de seguimiento
sea correcto o comp√°rteme m√°s detalles para ayudarte mejor.
"""

# -------------------------------------------------------------------
# Ejemplo de devoluci√≥n m√∫ltiple
# -------------------------------------------------------------------
multi_return_example = """
[Cliente] 2025-10-10: Quiero devolver los art√≠culos del pedido 45001.
[Agente]  2025-10-10: Con gusto. Para el pedido 45001:
- El "Pantal√≥n de lino azul" puede devolverse hasta el 20 de octubre de 2025.
- La "Camiseta blanca algod√≥n" tambi√©n es aceptada para devoluci√≥n.
- El "Set de velas arom√°ticas" no puede devolverse por pol√≠tica de productos
  fr√°giles y abiertos.
Si deseas continuar, puedo enviarte las instrucciones para devolver
los productos aceptados.
"""

# -------------------------------------------------------------------
# Ejemplo de consulta combinada (seguimiento + devoluci√≥n)
# -------------------------------------------------------------------
combined_example = """
[Cliente] 2025-10-12: Hola, quiero saber si ya lleg√≥ mi pedido 50123 y
tambi√©n si puedo devolver el "Paraguas rojo" de ese pedido.
[Agente]  2025-10-12: ¬°Hola! El pedido 50123 fue entregado el 11 de octubre
de 2025 en Bogot√°, Colombia por Servientrega.
En cuanto a la devoluci√≥n, el "Paraguas rojo" puede devolverse
hasta el 25 de octubre de 2025.
Si deseas iniciar el proceso, te env√≠o las indicaciones.
"""


## Automatizaci√≥n y pruebas

- Scripts de verificaci√≥n (`start.py`, `check_docker.py`) manejan reconstrucciones, detecci√≥n de puertos ocupados y fallos comunes.
- La validaci√≥n se realiza mediante inspecci√≥n est√°tica y pruebas manuales de la app.

## Ejemplos de pruebas

<img width="793" height="556" alt="image" src="https://github.com/user-attachments/assets/3dc7d6f1-cf9c-4a8e-8116-cec021215d7a" />

<img width="712" height="537" alt="image" src="https://github.com/user-attachments/assets/0fd99756-93b5-48da-9e67-a085eed37d8c" />

<img width="807" height="513" alt="image" src="https://github.com/user-attachments/assets/5adc0c9a-ee80-4034-b72a-9e6103e87a1f" />



