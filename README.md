# Taller AI Generativa
Este repositorio contiene el material y el cÃ³digo del taller â€œOptimizaciÃ³n de la AtenciÃ³n al Cliente en E-commerce con IA Generativaâ€, basado en el caso de estudio de la empresa ficticia "EcoMarket".


# Fase 1 â€“ SelecciÃ³n y JustificaciÃ³n del Modelo de IA

## 1ï¸âƒ£ Tipo de modelo propuesto
OptarÃ­a por un **LLM de propÃ³sito general** , en este caso el *Llama 3* combinado con un enfoque de **Retrieval-Augmented Generation (RAG)**.

- **Motivo principal:** La mayorÃ­a de las consultas (â‰ˆ 80 %) son repetitivas y dependen de informaciÃ³n actualizada del negocio (estado de pedidos, polÃ­ticas de devoluciÃ³n, catÃ¡logo).  
  Un RAG permite que el modelo base no requiera reentrenamiento cada vez que cambian los datos: solo necesita conectarse a una base de conocimiento dinÃ¡mica.


## 2ï¸âƒ£ Por quÃ© la elecciÃ³n del modelo
- **PrecisiÃ³n y fluidez:** Un LLM grande ofrece lenguaje natural y tono empÃ¡tico para responder en mÃºltiples canales (chat, email, redes sociales).
- **Costo / eficiencia:** En lugar de un *fine-tuned* propietario â€”que implicarÃ­a entrenamiento continuo y costo elevadoâ€” el enfoque RAG permite usar el modelo tal cual, complementado con datos internos indexados.
- **Escalabilidad:** El mismo modelo puede atender picos de miles de consultas diarias mediante *autoscaling* en la nube.
- **Privacidad:** Es posible tener un control total de los datos al usar con una infraestructura propia en la nube.


## 3ï¸âƒ£ Arquitectura propuesta  

```text
Usuarios (chat, email, redes sociales)
        â”‚
     Gateway + API
        â”‚
 â”Œâ”€â”€â”€â”€â”€Orquestador (LangChain + FastAPI)â”€â”€â”€â”€â”€â”
 â”‚        â€¢ Preprocesamiento de la pregunta  â”‚
 â”‚        â€¢ Llamado a motor de bÃºsqueda      â”‚
 â”‚        â€¢ DetecciÃ³n de consulta compleja   â”‚
 â”‚        â”œâ”€â”€> Si es compleja â†’ Agente Humanoâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
 Vector DB (FAISS)  â†  Datos EcoMarket
        â”‚
 LLM (Llama 3)  â†’  Respuesta generada
```
## âš™ï¸ TecnologÃ­as Seleccionadas y JustificaciÃ³n

### ğŸ§© Gateway + API
El **gateway** gestiona el trÃ¡fico y la seguridad; la **API (FastAPI)** recibe la solicitud y la pasa al orquestador.

- Esta separaciÃ³n permite **escalar** y **asegurar** el sistema, aunque en entornos pequeÃ±os ambos roles pueden estar en la misma aplicaciÃ³n FastAPI.

### ğŸ”— LangChain + FastAPI
Combina la **orquestaciÃ³n de flujos de IA** de **LangChain** con la **velocidad y facilidad de despliegue** de **FastAPI**, ofreciendo una API robusta, escalable y de alta disponibilidad para integrar el modelo con los distintos canales de comunicaciÃ³n.

### ğŸ—‚ï¸ FAISS
LibrerÃ­a **open source** optimizada para **bÃºsquedas vectoriales rÃ¡pidas**.  
Permite mantener **control total de los datos** y **reducir costos**, ideal para gestionar el Ã­ndice semÃ¡ntico de productos y pedidos sin depender de servicios de terceros.

### ğŸ¤– Llama 3
Modelo de lenguaje **open source** de alto rendimiento que brinda **privacidad y personalizaciÃ³n** al ejecutarse en una nube privada.  
Ofrece **calidad de generaciÃ³n cercana a GPT-4** con la flexibilidad de *fine-tuning* y **costos predecibles**.


- **IntegraciÃ³n con base de datos:**  
  - CatÃ¡logo de productos, informaciÃ³n de envÃ­os y estados de pedido se almacenan en una base de datos relacional/NoSQL y se indexan en un motor vectorial para consultas en tiempo real.  
  - El modelo consulta estos datos mediante RAG antes de redactar la respuesta.
- **Modo de operaciÃ³n:**  
  - Para preguntas de estado de pedido, una capa de lÃ³gica de negocio valida identidad y obtiene datos exactos.  
  - Para consultas complejas (20 %), el sistema deriva automÃ¡ticamente a un agente humano, entregando el contexto de la conversaciÃ³n.


## 4ï¸âƒ£ JustificaciÃ³n Final

- **Costo:**  
  Uso de un **LLM open source (Llama 3)** junto con un Ã­ndice vectorial **FAISS** permite **evitar tarifas por token** y mantener **costos predecibles**.  
  Se paga Ãºnicamente por la infraestructura en la nube, que puede dimensionarse segÃºn la demanda.

- **Escalabilidad:**  
  La arquitectura combina **Gateway + API (FastAPI)** con un orquestador en **LangChain**, facilitando el *autoscaling* en la nube.  
  Esto permite manejar miles de consultas simultÃ¡neas, con **balanceo de carga** y **cacheo de respuestas frecuentes** para optimizar recursos.

- **Mantenibilidad:**  
  La base de conocimiento se actualiza dinÃ¡micamente en **FAISS**, sin necesidad de reentrenar el modelo.  
  La separaciÃ³n de capas (Gateway, API, Orquestador y Vector DB) simplifica actualizaciones y despliegues continuos.

- **Privacidad y Control de Datos:**  
  **Llama 3** puede ejecutarse en una nube privada, asegurando que las conversaciones y datos de clientes permanezcan bajo control total, clave para cumplir regulaciones.

- **Calidad de Respuesta:**  
  **Llama 3**, integrado con LangChain, ofrece **lenguaje natural, empatÃ­a y soporte multicanal**, garantizando interacciones de alta calidad.  
  AdemÃ¡s, el orquestador identifica consultas complejas y **deriva automÃ¡ticamente a un agente humano** con el contexto completo para una atenciÃ³n sin fricciones.

---
